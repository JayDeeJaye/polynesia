{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Training Loop\n",
    "In Part 1, we put together a basic approach to implementing an algorithm for updating our Q function. We saw it work for one pass. Now, let's complete the loop and use it to search for an optimal policy.\n",
    "\n",
    "Our todo list:\n",
    "1. Refactor the experiment, pulling out reusable code into standalone methods\n",
    "1. Establish the terminating condition\n",
    "1. Implement a complete episode\n",
    "1. Finally, loop through a pre-determined number of episodes\n",
    "\n",
    "We're staying with random action selection for exploration for now. After we get the training loop going, we'll try using the evolving policy to select the next action.\n",
    "\n",
    "## Refactoring\n",
    "Below is our experiment with getAction, getUpdatedQsa, and getReward added to the utilites module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load utilities\n",
    "import random\n",
    "\n",
    "def hit(p,s):\n",
    "    p.receive(s.draw())\n",
    "    print(\"New hand: {} ({})\".format(p.hand,p.getPoints()))\n",
    "\n",
    "def newHand(d,p,s):\n",
    "    p.reset()\n",
    "    d.reset()\n",
    "    deal(d,p,s)\n",
    "\n",
    "# Deal\n",
    "def deal(d,p,s):\n",
    "    d.receive(s.draw())\n",
    "    p.receive(s.draw())\n",
    "    d.receive(s.draw())\n",
    "    p.receive(s.draw())\n",
    "    print(\"Dealer's hand: {} ({})\".format(d.hand,d.getPoints()))\n",
    "    print(\"Player's hand: {} ({})\".format(p.hand,p.getPoints()))\n",
    "    \n",
    "def getAction():\n",
    "    r = random.randint(0,1)\n",
    "    if r == 0:\n",
    "        return 'HIT'\n",
    "    else:\n",
    "        return 'STAY'\n",
    "\n",
    "# Updated Q(s,a) value\n",
    "def getUpdatedQsa(Q,s,r,sPrime,A):\n",
    "    return Q[s] + 0.08*(r + max(Q[sPrime+A[0:1]],Q[sPrime+A[1:2]]) - Q[s])\n",
    "\n",
    "# Calculate reward\n",
    "def getReward(p,d,isTerminal):\n",
    "\n",
    "    # Non-Terminal state rewards\n",
    "    if not isTerminal:\n",
    "    # Did the player bust?\n",
    "        if p.getPoints() > 21:\n",
    "            r = -1\n",
    "        else:\n",
    "            # Hand is still going\n",
    "            r = 0\n",
    "        return r\n",
    "    \n",
    "    # Other state rewards\n",
    "    # Did the player bust?\n",
    "    if p.getPoints() > 21:\n",
    "        r = -1\n",
    "    elif d.getPoints() > 21:\n",
    "        r = 1\n",
    "    elif (p.getPoints() > d.getPoints()):\n",
    "        r = 1\n",
    "    elif (p.getPoints() < d.getPoints()):\n",
    "        r = -1\n",
    "    else:\n",
    "        r = 0\n",
    "\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from player import Player\n",
    "from shoe import Shoe\n",
    "from utilities import hit, newHand, deal, getAction, getUpdatedQsa, getReward\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Initialize\n",
    "shoe = Shoe(1)\n",
    "dealer = Player()\n",
    "player = Player()\n",
    "allActions=('HIT','STAY',)\n",
    "Q = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starting state for a hand/episode\n",
    "newHand(dealer,player,shoe)\n",
    "\n",
    "# 1. Choose an action\n",
    "action = getAction()\n",
    "\n",
    "# 2. Observe the state\n",
    "currentState=(player.getPoints(),dealer.hand[0],action)\n",
    "print(\"Current state: {}\".format(currentState))\n",
    "\n",
    "# 3. Do the action\n",
    "if (action == 'HIT'):\n",
    "    print(\"Player {}: \".format(action), end=' ')\n",
    "    hit(player,shoe)\n",
    "newState = (player.getPoints(),dealer.hand[0])\n",
    "\n",
    "# Calculate reward\n",
    "if (action == 'STAY'):\n",
    "    while dealer.getPoints() < 17:\n",
    "        print(\"Dealer HIT: \", end=' ')\n",
    "        hit(dealer,shoe)\n",
    "    \n",
    "    reward = getReward(player,dealer,True)\n",
    "else:\n",
    "    reward = getReward(player,dealer,False)\n",
    "\n",
    "# 4. Update Q(s,a)\n",
    "Q[currentState] = getUpdatedQsa(Q,currentState,reward,newState,allActions)\n",
    "\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating Through the Episode\n",
    "If we take the approach of focusing on generating a converging policy, then we can use an algorithm which is dedicated to the Q update calculation. An example of such an algorithm, which we've been gravitating towards, is pictured below.\n",
    "\n",
    "```\n",
    "Initialize s\n",
    "Repeat\n",
    "    Choose a\n",
    "    Take action a, observe r, s'\n",
    "    Update Q(s,a)\n",
    "    s <-- s'\n",
    "Until s is terminal\n",
    "```\n",
    "\n",
    "This is reasonably straightforward, but we have some challenges in our Blackjack world. Let's take a look at what terminal states look like for us.\n",
    "\n",
    "### Terminal State\n",
    "Put simply, the terminal state is the end of the Blackjack hand. Blackjack hands end under these conditions:\n",
    "* The player goes bust (over 21)\n",
    "* The player stays and the dealer completes his turn\n",
    "* The dealer has Blackjack (21 in a hand with two cards)\n",
    "\n",
    "Clearly, the hand can end before it begins -- when the dealer gets Blackjack, it doesn't matter what anyone does. There's no action the player can take, there's no utility to calculate, there's nothing to learn. Because starting in the terminal state means the player loses all control, we should eliminate that from our algorithm. We do that by changing the loop control from a Repeat..Until construct to a While..Do construct.\n",
    "\n",
    "Let's create a method to implement these rules, taking a state-action pair (s,a), and the player and dealer objects as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isTerminalState(sa,p,d):\n",
    "    if sa[-1] == 'TERMINAL':\n",
    "        return True\n",
    "    \n",
    "    if p.getPoints() > 21:\n",
    "        return True\n",
    "    \n",
    "    if d.getPoints() == 21 and len(d.hand) == 2:\n",
    "        return True\n",
    "\n",
    "    if sa[-1] == 'STAY':\n",
    "        return True\n",
    "    \n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "Recall that the reward which results from taking action depends on if the hand is over or not. Sometimes the hand is over because of the state (the player has gone bust), and sometimes it is over because of the action (the player chooses to stand with the cards in her hand). We've got a terminal state dectector now, which won't change if the dealer plays out his hand. So the original reward calculation logic can be compressed to\n",
    "\n",
    "```\n",
    "if isTerminalState after action {\n",
    "    Dealer plays out hand\n",
    "}\n",
    "caculate reward\n",
    "```\n",
    "We're going to introduce two new descriptive actions, *NONE* and *TERMINAL*, to meet the requirements of having a complete (s,a) specification for the initial state and new state generations, and to signal to other modules the status of the episode. That means we'll need to make a slight adjustment to the updatedQsa function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing tuple slicing...\n",
    "myTuple = (16, 4, 'NONE')\n",
    "print(myTuple)\n",
    "print(myTuple[0:-1]) # give me everything but the last element of the tuple as a tuple\n",
    "\n",
    "def getUpdatedQsa(Q,s,r,sPrime,A):\n",
    "    return Q[s] + 0.08*(r + max(Q[sPrime[0:-1]+A[0:1]],Q[sPrime[0:-1]+A[1:2]]) - Q[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have all we need to implement the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    # Initialize s\n",
    "    newHand(dealer,player,shoe)\n",
    "    currentState = (player.getPoints(),dealer.hand[0],'NONE')\n",
    "\n",
    "    while not isTerminalState(currentState,player,dealer):\n",
    "\n",
    "        # Choose an action\n",
    "        action = getAction()\n",
    "        currentState=(player.getPoints(),dealer.hand[0],action)\n",
    "        print(\"Current state: {}\".format(currentState))\n",
    "\n",
    "        # Take the action\n",
    "        if (action == 'HIT'):\n",
    "            print(\"Player {}: \".format(action), end=' ')\n",
    "            hit(player,shoe)\n",
    "        newState = (player.getPoints(),dealer.hand[0],'NONE')\n",
    "\n",
    "        # Observe the new state and its reward. If the action taken or\n",
    "        # the new state is terminal, then we need to have the dealer \n",
    "        # play out to generate the reward\n",
    "        isTerminal = isTerminalState(currentState,player,dealer)\n",
    "        if (isTerminal):\n",
    "            newState = (player.getPoints(),dealer.hand[0],'TERMINAL')\n",
    "            while dealer.getPoints() < 17:\n",
    "                print(\"Dealer HIT: \", end=' ')\n",
    "                hit(dealer,shoe)\n",
    "\n",
    "        reward = getReward(player,dealer,isTerminal)\n",
    "\n",
    "        # Update Q(s,a)\n",
    "        Q[currentState] = getUpdatedQsa(Q,currentState,reward,newState,allActions)\n",
    "\n",
    "        currentState = newState\n",
    "    print(\"we're done.\")\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting invalid states in the Q space, hands that are over 21. Should I have them?\n",
    "\n",
    "I think maybe my algorithm isn't behaving like an agent. What happens if I do ask it to behave like an agent?\n",
    "\n",
    "The Q-LEARNING-AGENT function takes a percept (a state and a reward) as input and returns an action. Persistent values include Q, and (s,a,r) which are the previous state, action, and reward -- initially null.\n",
    "\n",
    "```\n",
    "function Q-LEARNING-AGENT(percept) returns an action\n",
    "  inputs: percept, a percept indicating the current state s' and reward signal r'\n",
    "  persisent: Q[s,a], a table of action values indexed by state and action, initially 0\n",
    "             N[s,a], a table of frequencies for state-action pairs, initially 0 (used to throttle exploration)\n",
    "             s, a, r, the previous state, action, and reward, initially null\n",
    "\n",
    "    if TERMINAL?(s') then Q[s',None] <-- r\n",
    "    if s is not null then\n",
    "        increment N[s,a]\n",
    "        Q[s,a] <-- Q[s,a] + lr * (N[s,a] * (r + discount * max_a'(Q[s',a']) - Q[s,a]))\n",
    "    s, a, r <-- s', argmax_a' f(Q[s',a'],N[s',a']), r'\n",
    "    return a\n",
    "```\n",
    "\n",
    "**This** we can repeat until s is terminal. Let's the code and fit this algorithm. We'll leave action choice with random, for now.\n",
    "\n",
    "We'll need another version of the terminal state test function. And the update function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isTerminal(s,d):\n",
    "    if s[0] > 21:\n",
    "        return True\n",
    "    \n",
    "    if d.getPoints() == 21 and len(d.hand) == 2:\n",
    "        return True\n",
    "\n",
    "# Staying can be identified elsewhere\n",
    "#    if a == 'STAY':\n",
    "#        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Updated Q(s,a) value\n",
    "def getUpdatedQsa(Q,ps,pa,pr,s):\n",
    "    return Q[ps+(pa,)] + 0.08*(pr + max(Q[s+('HIT',)],Q[s+('STAY',)]) - Q[ps+(pa,)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the experiment\n",
    "shoe = Shoe(1)\n",
    "dealer = Player()\n",
    "player = Player()\n",
    "allActions=('HIT','STAY',)\n",
    "Q = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealer's hand: [10, 4] (14)\n",
      "Player's hand: [9, 7] (16)\n",
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m<ipython-input-37-427df6d9b8c9>\u001b[0m(27)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     25 \u001b[0;31m\u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     26 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 27 \u001b[0;31m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     29 \u001b[0;31m\u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> b getUpdatedQsa\n",
      "Breakpoint 5 at <ipython-input-32-b308b8db0ea9>:15\n",
      "ipdb> b\n",
      "Num Type         Disp Enb   Where\n",
      "1   breakpoint   keep yes   at <ipython-input-22-5ecc7f574496>:29\n",
      "\tbreakpoint already hit 1 time\n",
      "2   breakpoint   keep yes   at <ipython-input-25-9b11f032abeb>:29\n",
      "\tbreakpoint already hit 1 time\n",
      "3   breakpoint   keep yes   at <ipython-input-30-9b11f032abeb>:29\n",
      "\tbreakpoint already hit 1 time\n",
      "4   breakpoint   keep yes   at <ipython-input-34-d258266e4472>:29\n",
      "\tbreakpoint already hit 1 time\n",
      "5   breakpoint   keep yes   at <ipython-input-32-b308b8db0ea9>:15\n",
      "ipdb> r\n",
      "> \u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m(2884)\u001b[0;36mrun_code\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   2882 \u001b[0;31m            \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   2883 \u001b[0;31m                \u001b[0;31m# Reset our crash handler in place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 2884 \u001b[0;31m                \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexcepthook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_excepthook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   2885 \u001b[0;31m        \u001b[0;32mexcept\u001b[0m \u001b[0mSystemExit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   2886 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> r\n",
      "--Return--\n",
      "0\n",
      "> \u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m(2901)\u001b[0;36mrun_code\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   2899 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   2900 \u001b[0;31m            \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 2901 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0moutflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   2902 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   2903 \u001b[0;31m    \u001b[0;31m# For backwards compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "HIT\n",
      "Player HIT:  New hand: [9, 7, 3] (19)\n",
      "> \u001b[0;32m<ipython-input-32-b308b8db0ea9>\u001b[0m(16)\u001b[0;36mgetUpdatedQsa\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     12 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m\u001b[0;31m# Updated Q(s,a) value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m5\u001b[0;32m    15 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mgetUpdatedQsa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 16 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.08\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HIT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'STAY'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "TypeError: unsupported operand type(s) for +: 'NoneType' and 'float'\n",
      "> \u001b[0;32m<ipython-input-32-b308b8db0ea9>\u001b[0m(16)\u001b[0;36mgetUpdatedQsa\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     12 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m\u001b[0;31m# Updated Q(s,a) value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m5\u001b[0;32m    15 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mgetUpdatedQsa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 16 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.08\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HIT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'STAY'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> p ps, pa, pr, s\n",
      "((16, 10), 'HIT', None, (19, 10))\n",
      "ipdb> q\n",
      "Exiting Debugger.\n"
     ]
    }
   ],
   "source": [
    "#from IPython.core.debugger import Tracer; debug_here = Tracer()\n",
    "from IPython.core.debugger import Pdb\n",
    "pdb = Pdb()\n",
    "def qLearningAgent(sp,rp,s,a,r,Q,p,d,terminal):\n",
    "\n",
    "    if terminal:\n",
    "        Q[sp+('NONE',)] = rp\n",
    "\n",
    "    if s is not None:\n",
    "        Q[s+(a,)] = getUpdatedQsa(Q,s,a,r,sp)\n",
    "\n",
    "    s = sp\n",
    "    a = getAction(); print(a)\n",
    "    r = rp\n",
    "\n",
    "    return (s,a,r)\n",
    "\n",
    "# Initialize s'\n",
    "newHand(dealer,player,shoe)\n",
    "currentState = (player.getPoints(),dealer.hand[0],)\n",
    "currentReward = None\n",
    "terminal = False\n",
    "s = None\n",
    "a = None\n",
    "r = None\n",
    "\n",
    "pdb.set_trace()\n",
    "\n",
    "while True:\n",
    "    # Choose an action\n",
    "    sar = qLearningAgent(currentState,currentReward,s,a,r,Q,player,dealer,terminal)\n",
    "    if terminal:\n",
    "        break\n",
    "    \n",
    "    s = sar[0]; a = sar[1]; r = sar[2]\n",
    "    # Take the action\n",
    "    if (a == 'HIT'):\n",
    "        print(\"Player {}: \".format(action), end=' ')\n",
    "        hit(player,shoe)\n",
    "        currentState = (player.getPoints(),dealer.hand[0],)\n",
    "        terminal = isTerminal(currentState,dealer)\n",
    "    else: \n",
    "        while dealer.getPoints() < 17:\n",
    "            print(\"Dealer HIT: \", end=' ')\n",
    "            hit(dealer,shoe)\n",
    "        terminal = True\n",
    "    \n",
    "    currentReward = getReward(player,dealer,terminal)\n",
    "\n",
    "\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {(13, 11, 'HIT'): 0.0,\n",
       "             (13, 11, 'NONE'): -1,\n",
       "             (13, 11, 'STAY'): 0.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
