{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Environment\n",
    "In Blackjack, the object is to win as many hands as possible over time, hopefully ending up with more wins than losses. As in any kind of gambling, the better one's chance of winning a hand, the better chance one has of continuing and therefore winning more hands.\n",
    "\n",
    "The starting state of any hand is the result of the deal. For the player, we know the total value the hand; the dealer's hand is dealt with the first card facing up and the second facing down. There are two actions the player can take:\n",
    "1. she can ask for another card (hit) and risk going bust (over 21 points), or \n",
    "2. accept her hand as it is (stand).\n",
    "\n",
    "The turn for the player ends when she either goes bust or chooses to stand.\n",
    "\n",
    "The dealer's rules for action are strict, allowing for no choice: \n",
    "1. As long as his hand totals 16 points or less, he must hit.\n",
    "1. He must stand if his hand totals 17 points or more.\n",
    "\n",
    "The hand ends after the dealer has a final score based on these rules, and the player finds out the consequences of her choice:\n",
    "* If the player's total is higher than the dealer's, the player wins.\n",
    "* If the dealer's total is higher than the player's, the player loses.\n",
    "* If the totals of the player and dealer are the same, the hand is a draw.\n",
    "\n",
    "## The Model\n",
    "* We can treat one hand as an episode for our learning algorithm.\n",
    "* Each state is a combination of the player's total points and the dealer's card which is showing.\n",
    "* There are two actions: hit and stand\n",
    "* Reward is assigned as follows:\n",
    "    * If the hand is over, and the player has won, then reward is +1\n",
    "    * If the hand is over, and the player has lost, then reward is -1\n",
    "    * Reward is 0 for all other states\n",
    "\n",
    "In Q-Learning, an optimal policy can be found by trying actions and keeping a record of performance. Moreover, because the environment is not completely known, the algorithm must explore possibilities and anticipate the unknown. We'll need some data structures for keeping track of actions with different states and their results.\n",
    "\n",
    "Python's dictionary structure is a good fit for that. We can use tuples to represent the state/action combinations. These will be indexes into the dictionary, which will store the utility and value functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(14, 5, 'STAY'): 0.0\n",
      "Q(14, 7, 'HIT'): 0.0\n",
      "Q(14, 7, 'STAY'): 0.0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "# Sample Q function implementation using tuples\n",
    "Q = defaultdict(float)\n",
    "# Couple of examples from basic strategy\n",
    "Q[14,5,'STAY']\n",
    "Q[14,7,'HIT']\n",
    "Q[14,7,'STAY']\n",
    "for k in sorted(Q.keys()):\n",
    "    print(\"Q{}: {}\".format(k,Q[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Drive\n",
    "Let's try a simple implementation of the basic Q-Learning algorithm. This algorithm builds a utility function Q that when finished is expected to represent the best action to take in a given state. As an action is taken in a state, the reward of the action and estimated future actions are stored as the value of Q(s,a). As the state and action repeat during the training phase, the value of Q(s,a) is updated:\n",
    "\n",
    "$$Q(s,a)=Q(s,a) + \\alpha(R(s) + \\gamma \\underset{a'}{\\operatorname{max}}Q(s',a') - Q(s,a))$$\n",
    "\n",
    "Without getting too far ahead of ourselves, let's try one step through the algorithm. Very simply, we'll do the initialization, start the hand, and let the player take one action, chosen at random. A stay will signal the dealer's turn, and we can see if the player wins the hand or not. The Q function gets updated once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from player import Player\n",
    "from shoe import Shoe\n",
    "from utilities import hit, newHand, deal\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def getAction():\n",
    "    r = random.randint(0,1)\n",
    "    if r == 0:\n",
    "        return 'HIT'\n",
    "    else:\n",
    "        return 'STAY'\n",
    "\n",
    "# Initialize\n",
    "shoe = Shoe(1)\n",
    "dealer = Player()\n",
    "player = Player()\n",
    "allActions=('HIT','STAY',)\n",
    "Q = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting round.py\n"
     ]
    }
   ],
   "source": [
    "# Starting state for a hand/episode\n",
    "newHand(dealer,player,shoe)\n",
    "\n",
    "# 1. Choose an action\n",
    "action = getAction()\n",
    "\n",
    "# 2. Observe the state\n",
    "currentState=(player.getPoints(),dealer.hand[0],action)\n",
    "print(\"Current state: {}\".format(currentState))\n",
    "\n",
    "# 3. Do the action\n",
    "if (action == 'HIT'):\n",
    "    print(\"Player {}: \".format(action), end=' ')\n",
    "    hit(player,shoe)\n",
    "newState = (player.getPoints(),dealer.hand[0])\n",
    "\n",
    "# Calculate reward\n",
    "if (action == 'STAY'):\n",
    "    while dealer.getPoints() < 17:\n",
    "        print(\"Dealer HIT: \", end=' ')\n",
    "        hit(dealer,shoe)\n",
    "    \n",
    "    if (player.getPoints() > dealer.getPoints()):\n",
    "        reward = 1\n",
    "    elif dealer.getPoints() > 21:\n",
    "        reward = 1\n",
    "    elif dealer.getPoints() == player.getPoints():\n",
    "        reward = 0\n",
    "    else: \n",
    "        reward = -1\n",
    "else:\n",
    "    if player.getPoints() > 21:\n",
    "        reward = -1\n",
    "    else:\n",
    "        reward = 0\n",
    "\n",
    "# 4. Update Q(s,a)\n",
    "Q[currentState] = Q[currentState] + 0.08*(reward + max(Q[newState+allActions[0:1]],Q[newState+allActions[1:2]]) - Q[currentState])\n",
    "\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula appears to be working ok. If the pass resulted in the end of the player's turn, either by staying or busting, the Q value was updated accordingly.\n",
    "\n",
    "Time to refactor and finish the loop.\n",
    "\n",
    "TODO:\n",
    "- Refactor\n",
    "  - Separate maxQa calculation into its own function\n",
    "  - Separate reward calculation into its own function\n",
    "  - Create a terminal state test\n",
    "- New features\n",
    "  - Terminal state flag\n",
    "  - Episode loop\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
