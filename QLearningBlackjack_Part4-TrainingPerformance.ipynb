{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back in Part 2, we tried building our Q updating function in two ways: as an iterative algorithm, and as an agent method. The iterative algorithm seemed to work ok, but it added Q indexes that wouldn't happen because they were after the terminal state. I'm not sure if that's a problem, really, except that if we're testing for convergence with something like MSR, it might throw the result off. Maybe not.\n",
    "\n",
    "Since then I've taken a look at some other game space implementations (needs reference to OpenAIGym blackjack.py environemtn), and Q-Learning algorithms (needs reference to https://gist.github.com/awjuliani/9024166ca08c489a60994e529484f7fe), and I think that the iterative approach wasn't bad after all. Let's go back to it.\n",
    "\n",
    "To review, this is what we hope to implement:\n",
    "\n",
    "```\n",
    "Initialize s\n",
    "Repeat\n",
    "    Choose a\n",
    "    Take action a, observe r, s'\n",
    "    Update Q(s,a)\n",
    "    s <-- s'\n",
    "Until s is terminal\n",
    "```\n",
    "Let's get to work. To do:\n",
    "\n",
    "* [x] We need to create the training loop for Q(s,a)\n",
    "\n",
    "   The training loop until now has been selecting actions which are purely random. This is an exploration-only policy, and it's a valid approach, but it doesn't help us converge on an optimal policy because there's no benefit of experience, or exploitation. \n",
    "\n",
    "\n",
    "* [x] So we need an exploration function\n",
    "\n",
    "   This is a function that decides when to choose randomly and when to choose based on past knowledge.\n",
    "   * We can simply try something random every n ticks of epoch\n",
    "   * We can introduce a threshold of exploration, which over time will decrease. As the probability that we've learned all we need to learn about a certain situation increases, because of the number of times we've visited it, we will go with what we know instead of trying something different. The only question now is, do we calculate the threshold based on state, or based on state,action?\n",
    "\n",
    "\n",
    "* [ ] We need a performance-measuring function. There are two choices:\n",
    "   * A loss function using expected $Q(s,a)=R(s) + \\gamma \\underset{a'}{\\operatorname{max}}Q(s',a')$\n",
    "   * A discounted total reward-to-go function $ R(t) = \\sum_{t=0}^{N} \\gamma^t r_t $, where t is any given time step in a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "# %load main.py\n",
    "from player import Player\n",
    "from shoe import Shoe\n",
    "from utilities import hit, newHand, deal, getReward, getAction, getUpdatedQsa\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#import logging\n",
    "#logging.basicConfig(filename='LearningPolicy.log', filemode='w', level=logging.DEBUG, format='%(asctime)s %(levelname)s:%(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "# Initialize\n",
    "shoe = Shoe(1)\n",
    "dealer = Player()\n",
    "player = Player()\n",
    "Q = defaultdict(float)\n",
    "N = defaultdict(float)\n",
    "RTG = [] # Reward to Go function\n",
    "CR = [] # Cumulative reward function\n",
    "LOSS = [] # Loss function\n",
    "ACTIONS=('HIT','STAND',)\n",
    "epsilon = 10\n",
    "lr = 0.08\n",
    "discount = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting round.py\n"
     ]
    }
   ],
   "source": [
    "# %load round.py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='LearningPolicy.log', filemode='w', level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "# Updated getAction() using exploration/exploitation\n",
    "def getAction(Q,N,s):\n",
    "    e = epsilon/(epsilon + N[s])\n",
    "    rr = random.random()\n",
    "    if rr < e:\n",
    "        logging.debug(\"Exploring (N[s] = {}, e = {}, rr = {}): Random action selected\".format(N[s],e,rr))\n",
    "        return ACTIONS[random.randint(0,1)]\n",
    "    else:\n",
    "        # Use what we've learned\n",
    "        logging.debug(\"Exploiting (N[s] = {}, e = {}, rr = {}): Optimal action selected\".format(N[s],e,rr))\n",
    "        if Q[s+('HIT',)] > Q[s+('STAND',)]:\n",
    "            return 'HIT'\n",
    "        else:\n",
    "            return 'STAND'\n",
    "\n",
    "def getUpdatedQsa(Q,sa,r,s1,A):\n",
    "    return Q[sa] + 0.08*(r + discount*max(Q[s1+A[0:1]],Q[s1+A[1:2]]) - Q[sa])\n",
    "\n",
    "def discountedRewardToGo(R):\n",
    "    pass\n",
    "\n",
    "# Intitalize first!\n",
    "# Train with a number of hands\n",
    "bankroll = 500.00\n",
    "netWins = 0\n",
    "wins = 0\n",
    "losses = 0\n",
    "pushes = 0\n",
    "WALLET=[]\n",
    "RMS=[]\n",
    "R=[0]\n",
    "n = 10000\n",
    "\n",
    "# print(\"Starting bankroll: {}\".format(bankroll))\n",
    "for i in range(n):\n",
    "    # Initialize s\n",
    "    newHand(dealer,player,shoe)\n",
    "    s = (player.getPoints(),dealer.hand[0],)\n",
    "    done = False\n",
    "    while True:\n",
    "        logging.debug(\"Current state: {}\".format(s))\n",
    "        # choose an action\n",
    "        a = getAction(Q,N,s)\n",
    "        logging.debug(\"Player {}: \".format(a))\n",
    "\n",
    "        # Take the action\n",
    "        if a == 'HIT':\n",
    "            hit(player,shoe)\n",
    "            if player.getPoints() > 21:\n",
    "                done = True\n",
    "        else:\n",
    "            # Player stands; play out the dealer\n",
    "            while dealer.getPoints() < 17:\n",
    "                logging.debug(\"Dealer HIT: \")\n",
    "                hit(dealer,shoe)\n",
    "            done = True\n",
    "        s1 = (player.getPoints(),dealer.hand[0],)\n",
    "\n",
    "        # Calculate the reward\n",
    "        if not done:\n",
    "            r = 0\n",
    "        else:\n",
    "            r = getReward(player,dealer)\n",
    "\n",
    "        # Update Q(s,a)\n",
    "        Q[s+(a,)] = getUpdatedQsa(Q,s+(a,),r,s1,ACTIONS)\n",
    "        N[s] += 1\n",
    "        s = s1    \n",
    "\n",
    "        # Stop if we're at the terminal state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Calculate and keep track of wins/losses\n",
    "    if r < 0:\n",
    "        logging.debug(\"Lose ({})\".format(r))\n",
    "        losses += 1\n",
    "    elif r > 0:\n",
    "        logging.debug(\"Win ({})\".format(r))\n",
    "        wins += 1\n",
    "    else:\n",
    "        logging.debug(\"Push ({})\".format(r))\n",
    "        pushes += 1\n",
    "\n",
    "    # Data from the episode    \n",
    "    bankroll += r*5\n",
    "    WALLET.append(bankroll)\n",
    "    R.append(r)\n",
    "    RMS.append(np.sqrt(sum((np.fromiter(iter(Q.values()), dtype=float))**2)/len(Q)))\n",
    "    RTG.append(sum([(R[t]*(discount**t)) for t in range(len(R))]))\n",
    "    CR.append(sum(R))\n",
    "\n",
    "logging.info(\"\\n\\tWins: {}\\n\\tLosses: {}\\n\\tPushes: {}\\n\\tOutcome: {}\".format(wins,losses,pushes,sum(R)))\n",
    "\n",
    "fig = plt.figure(1,figsize=(12,6))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot(CR)\n",
    "plt.title('Cumulative reward for {} hands'.format(n))\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(RTG)\n",
    "plt.title('Discounted Reward to Go for {} hands'.format(n))\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(WALLET)\n",
    "plt.title('Bankroll over {} hands assuming $5 bet'.format(n))\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(RMS)\n",
    "plt.title('Root Mean Square of Q for {} hands'.format(n))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = list(set([q[0:2] for q in iter(Q)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(7, 3, 'STAND'): -0.21137440986841682, (7, 3, 'HIT'): 0.1694807532392204}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7, 3, 'HIT')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = states[0]\n",
    "s\n",
    "qas = {q: Q[q] for q in Q.keys() if q[0:2] == s}\n",
    "print(qas)\n",
    "max(qas.keys(), key=lambda k: qas[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "f = open('QL_BasicStrategy_1.csv','w')\n",
    "writer = csv.writer(f)\n",
    "for s in states:\n",
    "    qas = {q: Q[q] for q in Q.keys() if q[0:2] == s}\n",
    "    oPolicy = max(qas.keys(), key=lambda k: qas[k])\n",
    "    if oPolicy[0] <= 21:\n",
    "        writer.writerow(list(oPolicy))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I've got a Q(s,a) policy after 10,000 training episodes, and I've stored it in the same format as the Basic Strategy. Plus, I have a graph that (I hope) shows a measure of the reward slope, smoothing out at any rate, even though it lands on the negative side. The environment isn't REALLY finished yet, I want to add splitting and doubling down to the mix.\n",
    "\n",
    "So, to do next:\n",
    "\n",
    "* Compare the learned policy with the optimal one\n",
    "* Run more training iterations, and compare the results. Don't forget to repeat and average results, at least 10 times\n",
    "* Decrease the learning rate over time. One option is to keep track of (s,a) and reduce the learning rate by a fraction each time it's used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
